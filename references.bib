@inproceedings{ijcnn2024_efficient_routing,
  author       = {{Zareapoor et al.}},
  title        = {Efficient Routing in Sparse Mixture-of-Experts},
  booktitle    = {Proceedings of the International Joint Conference on Neural Networks (IJCNN)},
  year         = {2024},
  doi          = {10.1109/ijcnn60899.2024.10650737}
}

@misc{he2024_million_experts,
  author       = {{He et al.}},
  title        = {Mixture of a Million Experts},
  year         = {2024},
  doi          = {10.48550/arxiv.2407.04153},
  archivePrefix= {arXiv},
  eprint       = {2407.04153}
}

@misc{yang2024_sparser_selection,
  author       = {{Yang et al.}},
  title        = {Enhancing Efficiency in Sparse Models with Sparser Selection},
  year         = {2024},
  doi          = {10.48550/arxiv.2403.18926},
  archivePrefix= {arXiv},
  eprint       = {2403.18926}
}

@inproceedings{li2018_resource_aware,
  author       = {{Li et al.}},
  title        = {Exploring Resource-Aware Deep Neural Network Accelerator and Architecture Design},
  booktitle    = {2018 International Conference on Digital Signal Processing (ICDSP)},
  year         = {2018},
  doi          = {10.1109/ICDSP.2018.8631853}
}

@misc{csordas2023_approx2ffn,
  author       = {{Csord{\'a}s et al.}},
  title        = {Approximating Two-Layer Feedforward Networks for Efficient Transformers},
  year         = {2023},
  doi          = {10.48550/arxiv.2310.10837},
  archivePrefix= {arXiv},
  eprint       = {2310.10837}
}

@misc{carolan2024_review_mmlm,
  author       = {{Carolan et al.}},
  title        = {A Review of Multi-Modal Large Language and Vision Models},
  year         = {2024},
  doi          = {10.48550/arxiv.2404.01322},
  archivePrefix= {arXiv},
  eprint       = {2404.01322}
}

@misc{gargbas2023_performance_mllm,
  author       = {Garg and Bas},
  title        = {On the Performance of Multimodal Language Models},
  year         = {2023},
  doi          = {10.48550/arxiv.2310.03211},
  archivePrefix= {arXiv},
  eprint       = {2310.03211}
}

@misc{bai2024_revolution_mllm,
  author       = {{Bai et al.}},
  title        = {The (R)Evolution of Multimodal Large Language Models: A Survey},
  year         = {2024},
  doi          = {10.48550/arxiv.2402.12451},
  archivePrefix= {arXiv},
  eprint       = {2402.12451}
}

@misc{niu2024_data_centric_mllm,
  author       = {{Niu et al.}},
  title        = {A Survey of Multimodal Large Language Model from A Data-centric Perspective},
  year         = {2024},
  doi          = {10.48550/arxiv.2405.16640},
  archivePrefix= {arXiv},
  eprint       = {2405.16640}
}

@misc{carolan2024_medical_mllm,
  author       = {{Carolan et al.}},
  title        = {From Text to Multimodality: Exploring the Evolution and Impact of Large Language Models in Medical Practice},
  year         = {2024},
  doi          = {10.31219/osf.io/7am8k}
}

@misc{jamba2024,
  author        = {{AI21 Labs}},
  title         = {Jamba: A Hybrid Transformer--Mamba Language Model},
  year          = {2024},
  doi           = {10.48550/arXiv.2403.19887},
  archivePrefix = {arXiv},
  eprint        = {2403.19887}
}